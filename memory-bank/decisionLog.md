# Decision Log

| Date | Decision | Rationale |
|------|----------|-----------|
| 2025-12-19 | Use DeepSeek function calling for LLM schema enforcement instead of simple JSON mode | The n8n production workflow uses LangChain's Structured Output Parser which internally uses function calling to enforce schemas. This is more reliable than response_format: json_object because the LLM must call the function with properly typed arguments. DeepSeek supports this via the tools API with tool_choice enforcement. Test proved this works perfectly - generated valid scenario in 45 seconds with all 11 expected_calculations fields populated. |
| 2025-12-19 | Use DeepSeek function calling for scenario schema enforcement instead of post-generation validation | n8n production workflow uses LangChain Structured Output Parser with autoFix: true. DeepSeek function calling with tools API provides equivalent functionality - the LLM is forced to use the generate_maac_scenario function which has the complete JSON schema defined. This ensures scenarios match the exact structure at generation time rather than requiring post-processing fixes. More reliable than template validation. |
| 2025-12-19 | Embed complete example scenario in system prompt rather than separate examples file | n8n workflow includes a complete example scenario (problem_solving-simple-111111111111-rep31) directly in the system prompt. This ensures the LLM sees the exact expected format with all fields populated correctly, including 12 expected_calculations and 6 success_thresholds. More effective than abstract schema descriptions - the LLM can pattern-match the example structure. |
| 2025-12-19 | Store scenarios to database immediately upon generation rather than batch-then-store | API endpoint /scenarios/generate-llm-stream uses SSE to stream progress updates while generating scenarios with the LLM, then stores completed scenarios to maac_experiment_scenarios table in batches of 50. This provides real-time feedback to users and ensures scenarios are persisted incrementally, reducing risk of data loss if generation is interrupted. Scenarios are immediately available for MIMIC engine retrieval. |
| 2025-12-19 | MIMIC Cognitive Engine has complete Memory Engine implementation with Graphiti integration | Audit revealed that packages/mimic-cognitive-engine contains fully implemented Memory Engine (632 lines) with: 1) Complete Graphiti HTTP client for store/query operations, 2) LLM-based memory classification into 5 types (procedural, episodic, semantic, declarative, reflective), 3) Three query endpoints (context, reflection, evaluation), 4) Full integration with MIMIC orchestrator as tools. The memory system was NOT missing - it exists in the separate mimic-cognitive-engine package which is a proprietary submodule. |
